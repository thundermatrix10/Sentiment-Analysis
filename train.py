import config
import dataset
import engine
import torch
import pandas as pd
import torch.nn as nn
import numpy as np
from sklearn.metrics import classification_report 
from model import BERTBaseUncased
from sklearn import model_selection
from sklearn import metrics
from transformers import AdamW
from transformers import get_linear_schedule_with_warmup

def reduce_mem_usage(df):

    start_mem = df.memory_usage().sum() / 1024**2
    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))

    for col in df.columns:
        col_type = df[col].dtype
        
        if col_type != object:
            c_min = df[col].min()
            c_max = df[col].max()
            if str(col_type)[:3] == 'int':
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                    df[col] = df[col].astype(np.int16)
                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                    df[col] = df[col].astype(np.int32)
                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:
                    df[col] = df[col].astype(np.int64)  
            else:
                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:
                    df[col] = df[col].astype(np.float16)
                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                    df[col] = df[col].astype(np.float32)
                else:
                    df[col] = df[col].astype(np.float64)
        else:
            pass

    end_mem = df.memory_usage().sum() / 1024**2
    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))
    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))
    
    return df

def run():
    i = 1
    for TRAINING_FILE in config.files:
        print("Training Dataset: ",i)
        i+=1
        dfx = pd.read_csv(TRAINING_FILE).fillna("none")
        dfx = dfx.sample(n=100)
        dfx = reduce_mem_usage(dfx)

        df_train, df_valid = model_selection.train_test_split(
            dfx,
            test_size=0.1,
            random_state=42,
            stratify=dfx.note.values
        )

        df_train = df_train.reset_index(drop=True)
        df_valid = df_valid.reset_index(drop=True)

        train_dataset = dataset.BERTDataset(
            review=df_train.review.values,
            target=df_train.note.values
        )

        train_data_loader = torch.utils.data.DataLoader(
            train_dataset,
            batch_size=config.TRAIN_BATCH_SIZE,
            num_workers=4
        )

        valid_dataset = dataset.BERTDataset(
            review=df_valid.review.values,
            target=df_valid.note.values
        )

        valid_data_loader = torch.utils.data.DataLoader(
            valid_dataset,
            batch_size=config.VALID_BATCH_SIZE,
            num_workers=1
        )

        device = torch.device("cpu")
        model = BERTBaseUncased()
        model.to(device)
        
        param_optimizer = list(model.named_parameters())
        no_decay = ["bias", "LayerNorm.bias", "LayerNorm.weight"]
        optimizer_parameters = [
            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},
            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},
        ]

        num_train_steps = int(len(df_train) / config.TRAIN_BATCH_SIZE * config.EPOCHS)
        optimizer = AdamW(optimizer_parameters, lr=3e-5,no_deprecation_warning=True)
        scheduler = get_linear_schedule_with_warmup(
            optimizer,
            num_warmup_steps=0,
            num_training_steps=num_train_steps
        )

        model = nn.DataParallel(model)

        best_accuracy = 0
        for epoch in range(config.EPOCHS):
            engine.train_fn(train_data_loader, model, optimizer, device, scheduler)
            outputs, targets = engine.eval_fn(valid_data_loader, model, device)
            outputs = np.array(outputs) >= 0.5
            accuracy = metrics.accuracy_score(targets, outputs)
            print(f"Accuracy Score = {accuracy}")

            predictions = [1 if output > 0.5 else 0 for output in outputs]

            report = classification_report(targets, predictions)

            print("Classification Report:")
            print(report)

            if accuracy > best_accuracy:
                torch.save(model.state_dict(), config.MODEL_PATH)
                best_accuracy = accuracy


if __name__ == "__main__":
    run()
